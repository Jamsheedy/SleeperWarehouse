{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e08b657-62c1-4fd6-b7b0-0d3d35f81ed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table('sleeper.bronze_players')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c621a971-3c8a-45ed-8532-a704bd4d6ac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8502acee-bc8a-4f91-9064-c5ccc6488a10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.bronze_rosters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b17c74-99c2-4d26-85a1-9ccec1ec7aa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, expr, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd81b298-ba4e-42f2-bdd7-cf614d87742f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"sleeper.bronze_rosters\") \\\n",
    "    .withColumn(\"player_id\", explode(\"players\")) \\\n",
    "    .withColumn(\"is_starter\", expr(\"array_contains(starters, player_id)\")) \\\n",
    "    .withColumn(\"player_nickname\", expr(\"metadata['p_nick_' || player_id]\"))\n",
    "\n",
    "df = df.select(\"owner_id\", \"roster_id\", \"player_id\", \"is_starter\", \"player_nickname\", \"_league_id\", \"_matchup_week\", \"_year\", \"_ingested_ts\")\\\n",
    "    .withColumn(\"_snapshot_ts\", current_timestamp())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f3a658d-7164-4177-93bc-ffd77d4b500f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.bronze_rosters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f22f7a31-3504-4ea4-84b5-f5b99ce1644d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"sleeper.bronze_rosters\") \\\n",
    "    .withColumn(\"streak\", expr(\"metadata['streak']\")) \\\n",
    "    .withColumn(\"record\", expr(\"metadata['record']\"))\\\n",
    "    .withColumn(\"wins\", expr(\"settings['wins']\"))\\\n",
    "    .withColumn(\"losses\", expr(\"settings['losses']\"))\\\n",
    "    .withColumn(\"ties\", expr(\"settings['ties']\"))\\\n",
    "    .withColumn(\"fpts\", expr(\"settings['fpts'] + settings['fpts_decimal'] / 100\")) \\\n",
    "    .withColumn(\"fpts_against\", expr(\"settings['fpts_against'] + settings['fpts_against_decimal'] / 100\")) \\\n",
    "    .withColumn(\"total_moves\", expr(\"settings['total_moves']\")) \\\n",
    "    .withColumn(\"waiver_budget_used\", expr(\"settings['waiver_budget_used']\")) \\\n",
    "    .withColumn(\"waiver_position\", expr(\"settings['waiver_position']\"))\n",
    "\n",
    "df = df.select(\n",
    "    \"owner_id\", \n",
    "    \"roster_id\",\n",
    "    \"streak\",\n",
    "    \"record\",\n",
    "    \"wins\",\n",
    "    \"losses\",\n",
    "    \"ties\",\n",
    "    \"fpts\",\n",
    "    \"fpts_against\",\n",
    "    \"total_moves\",\n",
    "    \"waiver_budget_used\",\n",
    "    \"waiver_position\",\n",
    "    \"_league_id\",\n",
    "    \"_matchup_week\",\n",
    "    \"_year\",\n",
    "    \"_ingested_ts\",\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01c6a633-edd9-4b6e-a249-eb2b83a85bfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.bronze_matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c474c9b7-26d3-4173-9710-55cf2f0c5b39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table('sleeper.bronze_matchups')\\\n",
    "    .select(\n",
    "        \"matchup_id\",\n",
    "        \"roster_id\",\n",
    "        \"points\",\n",
    "        \"_league_id\",\n",
    "        \"_matchup_week\",\n",
    "        \"_year\",\n",
    "        \"_ingested_ts\"\n",
    "    ).withColumn(\"_snapshot_ts\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "793c8a3e-7813-412d-b0bf-51e58c212e3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.bronze_matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ef5da31-1265-4d2d-b392-d55b9cd1c54e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains, col, explode\n",
    "\n",
    "df = spark.read.table('sleeper.bronze_matchups') \\\n",
    "    .withColumn(\"player_id\", explode(col(\"players\"))) \\\n",
    "    .withColumn(\"is_starter\", array_contains(col(\"starters\"), col(\"player_id\"))) \\\n",
    "    .withColumn(\"player_points\", col(\"players_points\")[col(\"player_id\")])\n",
    "\n",
    "df = df.select(\n",
    "    \"roster_id\",\n",
    "    \"matchup_id\",\n",
    "    \"player_id\",\n",
    "    \"player_points\",\n",
    "    \"is_starter\",\n",
    "    \"_league_id\",\n",
    "    \"_matchup_week\",\n",
    "    \"_year\",\n",
    "    \"_ingested_ts\",\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf88dc83-450b-483f-89ca-61ddbba8383c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.bronze_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "670a9d6c-0e28-4473-8ba8-2b88d168fc05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table('sleeper.bronze_users')\\\n",
    "    .withColumnRenamed(\"display_name\", \"owner_name\")\\\n",
    "    .withColumnRenamed(\"user_id\", \"owner_id\")\\\n",
    "    .withColumnRenamed(\"is_owner\", \"is_commissioner\")\\\n",
    "    .withColumn(\"team_name\", col(\"metadata.team_name\"))\n",
    "\n",
    "df = df.select(\n",
    "    \"owner_id\",\n",
    "    \"owner_name\",\n",
    "    \"is_bot\",\n",
    "    \"is_commissioner\",\n",
    "    \"team_name\",\n",
    "    \"_league_id\",\n",
    "    \"_matchup_week\",\n",
    "    \"_year\",\n",
    "    \"_ingested_ts\"\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57a9dfa2-0749-4e7e-87c6-37f67079f573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.silver_matchups_players_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd213ec4-4bc4-43dd-bcb9-1a75cf400629",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws, coalesce, when\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_matchups_players_dim = spark.read.table('sleeper.silver_matchups_players_dim')\n",
    "df_players_dim = spark.read.table('sleeper.silver_players_dim')\n",
    "\n",
    "df_players_dim = df_players_dim.select(\n",
    "    \"player_id\",\n",
    "    \"_league_id\",\n",
    "    \"_matchup_week\",\n",
    "    coalesce(col(\"full_name\"), col(\"last_name\")).alias(\"player_name\"),\n",
    "    col(\"position\").alias(\"player_position\"),\n",
    "    col(\"team\").alias(\"nfl_team\"),\n",
    "    \"years_exp\",\n",
    "    \"injury_status\",\n",
    "    concat_ws(\" \", col(\"injury_body_part\"), col(\"injury_notes\")).alias(\"injury_notes\"),\n",
    "    \"college\",\n",
    "    when(col(\"years_exp\") == 1, True).otherwise(False).alias(\"is_rookie\")\n",
    ")\n",
    "\n",
    "df_joined = df_matchups_players_dim.join(\n",
    "    df_players_dim,\n",
    "    (df_matchups_players_dim.player_id == df_players_dim.player_id) &\n",
    "    (df_matchups_players_dim._league_id == df_players_dim._league_id) &\n",
    "    (df_matchups_players_dim._matchup_week == df_players_dim._matchup_week)\n",
    ")\n",
    "\n",
    "window_spec = Window.partitionBy(\n",
    "    df_matchups_players_dim[\"_league_id\"],\n",
    "    df_matchups_players_dim[\"_matchup_week\"],\n",
    "    \"player_position\"\n",
    ").orderBy(\"player_points\")\n",
    "\n",
    "df_joined = df_joined.withColumn(\"position_points_percentile\", F.percent_rank().over(window_spec))\n",
    "\n",
    "df_joined = df_joined.select(\n",
    "    \"roster_id\",\n",
    "    \"matchup_id\",\n",
    "    df_matchups_players_dim.player_id,\n",
    "    \"player_name\",\n",
    "    \"player_position\",\n",
    "    \"nfl_team\",\n",
    "    \"player_points\",\n",
    "    \"position_points_percentile\",\n",
    "    \"is_starter\",\n",
    "    \"years_exp\",\n",
    "    \"is_rookie\",\n",
    "    \"injury_status\",\n",
    "    \"injury_notes\",\n",
    "    \"college\",\n",
    "    df_matchups_players_dim._league_id,\n",
    "    df_matchups_players_dim._matchup_week,\n",
    "    df_matchups_players_dim._year\n",
    ")\n",
    "\n",
    "display(df_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72b1f9af-3c7d-42b1-87ba-bb17167d00cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.silver_matchups_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc6c2af0-5a03-4df0-8244-2106d3748504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_matchups_fact = spark.read.table('sleeper.silver_matchups_fact')\n",
    "df_rosters_dim = spark.read.table('sleeper.silver_rosters_dim')\n",
    "df_users_dim = spark.read.table('sleeper.silver_users_dim')\n",
    "\n",
    "df_result = df_matchups_fact.join(\n",
    "    df_rosters_dim,\n",
    "    (df_matchups_fact._league_id == df_rosters_dim._league_id) &\n",
    "    (df_matchups_fact.roster_id == df_rosters_dim.roster_id) &\n",
    "    (df_matchups_fact._matchup_week == df_rosters_dim._matchup_week)\n",
    ").join(\n",
    "    df_users_dim,\n",
    "    (df_matchups_fact._league_id == df_users_dim._league_id) &\n",
    "    (df_rosters_dim.owner_id == df_users_dim.owner_id) &\n",
    "    (df_matchups_fact._matchup_week == df_users_dim._matchup_week)\n",
    ").select(\n",
    "    \"matchup_id\",\n",
    "    df_matchups_fact.roster_id,\n",
    "    df_users_dim.owner_id,\n",
    "    df_users_dim.owner_name,\n",
    "    df_users_dim.is_commissioner,\n",
    "    df_users_dim.team_name,\n",
    "    df_matchups_fact.points,\n",
    "    df_rosters_dim.streak,\n",
    "    df_rosters_dim.record,\n",
    "    df_rosters_dim.wins,\n",
    "    df_rosters_dim.losses,\n",
    "    df_rosters_dim.ties,\n",
    "    df_rosters_dim.fpts,\n",
    "    df_rosters_dim.fpts_against,\n",
    "    df_rosters_dim.waiver_budget_used,\n",
    "    df_rosters_dim.waiver_position,\n",
    "    df_matchups_fact._league_id,\n",
    "    df_matchups_fact._matchup_week,\n",
    "    df_matchups_fact._year\n",
    ")\n",
    "\n",
    "display(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0028c222-c143-4d7b-bbf7-db73d2a25b32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.silver_model_roster_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1069416-7396-4999-8557-d81f4324b00a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.silver_model_player_performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "088da45f-6b2e-4dd5-b8c3-278648fa1d31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_roster_results = spark.read.table('sleeper.silver_model_roster_results')\n",
    "df_player_performances = spark.read.table('sleeper.silver_model_player_performances')\n",
    "\n",
    "df_joined_results = df_roster_results.alias(\"roster\").join(\n",
    "    df_player_performances.alias(\"performance\"),\n",
    "    (col(\"roster.roster_id\") == col(\"performance.roster_id\")) &\n",
    "    (col(\"roster.matchup_id\") == col(\"performance.matchup_id\")) &\n",
    "    (col(\"roster._matchup_week\") == col(\"performance._matchup_week\"))\n",
    ")\n",
    "\n",
    "# Aggregate starter points and bench points\n",
    "df_aggregated = df_joined_results.groupBy(\n",
    "    \"roster.matchup_id\",\n",
    "    \"roster.roster_id\",\n",
    "    \"roster.owner_id\",\n",
    "    \"roster.owner_name\",\n",
    "    \"roster.is_commissioner\",\n",
    "    \"roster.team_name\",\n",
    "    \"roster.points\",\n",
    "    \"roster.streak\",\n",
    "    \"roster.record\",\n",
    "    \"roster.wins\",\n",
    "    \"roster.losses\",\n",
    "    \"roster.ties\",\n",
    "    \"roster.fpts\",\n",
    "    \"roster.fpts_against\",\n",
    "    \"roster.waiver_budget_used\",\n",
    "    \"roster.waiver_position\",\n",
    "    \"roster._league_id\",\n",
    "    \"roster._matchup_week\",\n",
    "    \"roster._year\"\n",
    ").agg(\n",
    "    F.sum(F.when(col(\"performance.is_starter\"), col(\"performance.player_points\")).otherwise(0)).alias(\"starter_points\"),\n",
    "    F.sum(F.when(~col(\"performance.is_starter\"), col(\"performance.player_points\")).otherwise(0)).alias(\"bench_points\")\n",
    ")\n",
    "\n",
    "# Find bench players who scored more than starters in the same position\n",
    "window_spec = Window.partitionBy(\n",
    "    \"performance.matchup_id\", \"performance.roster_id\", \"performance.player_position\"\n",
    ").orderBy(F.desc(\"performance.player_points\"))\n",
    "\n",
    "df_ranked = df_joined_results.withColumn(\"rank\", F.rank().over(window_spec))\n",
    "\n",
    "df_bench_better_than_starters = df_ranked.filter(\n",
    "    (col(\"rank\") == 1) & (~col(\"performance.is_starter\"))\n",
    ").select(\n",
    "    \"performance.matchup_id\",\n",
    "    \"performance.roster_id\",\n",
    "    \"performance.player_id\",\n",
    "    \"performance.player_name\",\n",
    "    \"performance.player_position\",\n",
    "    \"performance.player_points\"\n",
    ")\n",
    "\n",
    "# Join back to find the corresponding starter\n",
    "df_starters = df_joined_results.filter(col(\"performance.is_starter\")).select(\n",
    "    \"performance.matchup_id\",\n",
    "    \"performance.roster_id\",\n",
    "    \"performance.player_position\",\n",
    "    col(\"performance.player_name\").alias(\"starter_player_name\"),\n",
    "    col(\"performance.player_points\").alias(\"starter_player_points\")\n",
    ")\n",
    "\n",
    "df_bench_better_than_starters = df_bench_better_than_starters.join(\n",
    "    df_starters,\n",
    "    (df_bench_better_than_starters.matchup_id == df_starters.matchup_id) &\n",
    "    (df_bench_better_than_starters.roster_id == df_starters.roster_id) &\n",
    "    (df_bench_better_than_starters.player_position == df_starters.player_position)\n",
    ").select(\n",
    "    df_bench_better_than_starters.matchup_id,\n",
    "    df_bench_better_than_starters.roster_id,\n",
    "    F.struct(\n",
    "        df_bench_better_than_starters.player_name.alias(\"benched_player_name\"),\n",
    "        df_bench_better_than_starters.player_points.alias(\"benched_player_points\"),\n",
    "        df_starters.starter_player_name,\n",
    "        df_starters.starter_player_points,\n",
    "        (df_bench_better_than_starters.player_points - df_starters.starter_player_points).alias(\"point_opportunity_cost\")\n",
    "    ).alias(\"bench_better_than_starter\")\n",
    ")\n",
    "\n",
    "# Aggregate the structs into a list and sum point_opportunity_cost\n",
    "df_bench_better_than_starters_agg = df_bench_better_than_starters.groupBy(\n",
    "    \"matchup_id\", \"roster_id\"\n",
    ").agg(\n",
    "    F.collect_list(\"bench_better_than_starter\").alias(\"bench_better_than_starters\"),\n",
    "    F.sum(\"bench_better_than_starter.point_opportunity_cost\").alias(\"missed_starter_points\")\n",
    ")\n",
    "\n",
    "# Join the aggregated bench better than starters to df_aggregated\n",
    "df_aggregated = df_aggregated.join(\n",
    "    df_bench_better_than_starters_agg,\n",
    "    [\"matchup_id\", \"roster_id\"],\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Create a column for highest scoring players (top 3 performing players)\n",
    "window_spec_highest_scoring = Window.partitionBy(\"roster.matchup_id\", \"roster.roster_id\").orderBy(F.desc(\"performance.player_points\"))\n",
    "\n",
    "df_highest_scoring = df_joined_results.withColumn(\"rank\", F.row_number().over(window_spec_highest_scoring)).filter(col(\"rank\") <= 3)\n",
    "\n",
    "df_highest_scoring_agg = df_highest_scoring.groupBy(\"roster.matchup_id\", \"roster.roster_id\").agg(\n",
    "    F.collect_list(\n",
    "        F.struct(\n",
    "            col(\"performance.player_name\").alias(\"highest_scoring_player_name\"),\n",
    "            col(\"performance.player_points\").alias(\"highest_scoring_player_points\")\n",
    "        )\n",
    "    ).alias(\"highest_scoring_players\")\n",
    ")\n",
    "\n",
    "df_aggregated = df_aggregated.join(\n",
    "    df_highest_scoring_agg,\n",
    "    [\"matchup_id\", \"roster_id\"],\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Add opponent points\n",
    "df_opponent_points = df_aggregated.select(\n",
    "    col(\"matchup_id\"),\n",
    "    col(\"roster_id\").alias(\"opponent_roster_id\"),\n",
    "    col(\"starter_points\").alias(\"opponent_starter_points\")\n",
    ")\n",
    "\n",
    "df_aggregated = df_aggregated.join(\n",
    "    df_opponent_points,\n",
    "    (df_aggregated.matchup_id == df_opponent_points.matchup_id) &\n",
    "    (df_aggregated.roster_id != df_opponent_points.opponent_roster_id),\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Add coulve_won_with_bench column\n",
    "df_aggregated = df_aggregated.withColumn(\n",
    "    \"couldve_won_with_missed_bench_points\",\n",
    "    F.when(\n",
    "        col(\"starter_points\") > col(\"opponent_starter_points\"),\n",
    "        None\n",
    "    ).when(\n",
    "        (col(\"starter_points\") + col(\"missed_starter_points\")) > col(\"opponent_starter_points\"),\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "display(df_aggregated)\n",
    "df_aggregated.printSchema()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5782151327188749,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "develop_models",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
