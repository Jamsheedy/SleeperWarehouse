{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e08b657-62c1-4fd6-b7b0-0d3d35f81ed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table('sleeper.bronze_players')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c621a971-3c8a-45ed-8532-a704bd4d6ac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8502acee-bc8a-4f91-9064-c5ccc6488a10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.bronze_rosters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b17c74-99c2-4d26-85a1-9ccec1ec7aa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, expr, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd81b298-ba4e-42f2-bdd7-cf614d87742f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"sleeper.bronze_rosters\") \\\n",
    "    .withColumn(\"player_id\", explode(\"players\")) \\\n",
    "    .withColumn(\"is_starter\", expr(\"array_contains(starters, player_id)\")) \\\n",
    "    .withColumn(\"player_nickname\", expr(\"metadata['p_nick_' || player_id]\"))\n",
    "\n",
    "df = df.select(\"owner_id\", \"roster_id\", \"player_id\", \"is_starter\", \"player_nickname\", \"_league_id\", \"_matchup_week\", \"_year\", \"_ingested_ts\")\\\n",
    "    .withColumn(\"_snapshot_ts\", current_timestamp())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f3a658d-7164-4177-93bc-ffd77d4b500f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.bronze_rosters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f22f7a31-3504-4ea4-84b5-f5b99ce1644d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"sleeper.bronze_rosters\") \\\n",
    "    .withColumn(\"streak\", expr(\"metadata['streak']\")) \\\n",
    "    .withColumn(\"record\", expr(\"metadata['record']\"))\\\n",
    "    .withColumn(\"wins\", expr(\"settings['wins']\"))\\\n",
    "    .withColumn(\"losses\", expr(\"settings['losses']\"))\\\n",
    "    .withColumn(\"ties\", expr(\"settings['ties']\"))\\\n",
    "    .withColumn(\"fpts\", expr(\"settings['fpts'] + settings['fpts_decimal'] / 100\")) \\\n",
    "    .withColumn(\"fpts_against\", expr(\"settings['fpts_against'] + settings['fpts_against_decimal'] / 100\")) \\\n",
    "    .withColumn(\"total_moves\", expr(\"settings['total_moves']\")) \\\n",
    "    .withColumn(\"waiver_budget_used\", expr(\"settings['waiver_budget_used']\")) \\\n",
    "    .withColumn(\"waiver_position\", expr(\"settings['waiver_position']\"))\n",
    "\n",
    "df = df.select(\n",
    "    \"owner_id\", \n",
    "    \"roster_id\",\n",
    "    \"streak\",\n",
    "    \"record\",\n",
    "    \"wins\",\n",
    "    \"losses\",\n",
    "    \"ties\",\n",
    "    \"fpts\",\n",
    "    \"fpts_against\",\n",
    "    \"total_moves\",\n",
    "    \"waiver_budget_used\",\n",
    "    \"waiver_position\",\n",
    "    \"_league_id\",\n",
    "    \"_matchup_week\",\n",
    "    \"_year\",\n",
    "    \"_ingested_ts\",\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01c6a633-edd9-4b6e-a249-eb2b83a85bfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.bronze_matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c474c9b7-26d3-4173-9710-55cf2f0c5b39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table('sleeper.bronze_matchups')\\\n",
    "    .select(\n",
    "        \"matchup_id\",\n",
    "        \"roster_id\",\n",
    "        \"points\",\n",
    "        \"_league_id\",\n",
    "        \"_matchup_week\",\n",
    "        \"_year\",\n",
    "        \"_ingested_ts\"\n",
    "    ).withColumn(\"_snapshot_ts\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "793c8a3e-7813-412d-b0bf-51e58c212e3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.bronze_matchups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ef5da31-1265-4d2d-b392-d55b9cd1c54e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains, col, explode\n",
    "\n",
    "df = spark.read.table('sleeper.bronze_matchups') \\\n",
    "    .withColumn(\"player_id\", explode(col(\"players\"))) \\\n",
    "    .withColumn(\"is_starter\", array_contains(col(\"starters\"), col(\"player_id\"))) \\\n",
    "    .withColumn(\"player_points\", col(\"players_points\")[col(\"player_id\")])\n",
    "\n",
    "df = df.select(\n",
    "    \"roster_id\",\n",
    "    \"matchup_id\",\n",
    "    \"player_id\",\n",
    "    \"player_points\",\n",
    "    \"is_starter\",\n",
    "    \"_league_id\",\n",
    "    \"_matchup_week\",\n",
    "    \"_year\",\n",
    "    \"_ingested_ts\",\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf88dc83-450b-483f-89ca-61ddbba8383c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.bronze_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "670a9d6c-0e28-4473-8ba8-2b88d168fc05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table('sleeper.bronze_users')\\\n",
    "    .withColumnRenamed(\"display_name\", \"owner_name\")\\\n",
    "    .withColumnRenamed(\"user_id\", \"owner_id\")\\\n",
    "    .withColumnRenamed(\"is_owner\", \"is_commissioner\")\\\n",
    "    .withColumn(\"team_name\", col(\"metadata.team_name\"))\n",
    "\n",
    "df = df.select(\n",
    "    \"owner_id\",\n",
    "    \"owner_name\",\n",
    "    \"is_bot\",\n",
    "    \"is_commissioner\",\n",
    "    \"team_name\",\n",
    "    \"_league_id\",\n",
    "    \"_matchup_week\",\n",
    "    \"_year\",\n",
    "    \"_ingested_ts\"\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57a9dfa2-0749-4e7e-87c6-37f67079f573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.silver_matchups_players_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd213ec4-4bc4-43dd-bcb9-1a75cf400629",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws, coalesce, when\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_matchups_players_dim = spark.read.table('sleeper.silver_matchups_players_dim')\n",
    "df_players_dim = spark.read.table('sleeper.silver_players_dim')\n",
    "\n",
    "df_players_dim = df_players_dim.select(\n",
    "    \"player_id\",\n",
    "    \"_league_id\",\n",
    "    \"_matchup_week\",\n",
    "    coalesce(col(\"full_name\"), col(\"last_name\")).alias(\"player_name\"),\n",
    "    col(\"position\").alias(\"player_position\"),\n",
    "    col(\"team\").alias(\"nfl_team\"),\n",
    "    \"years_exp\",\n",
    "    \"injury_status\",\n",
    "    concat_ws(\" \", col(\"injury_body_part\"), col(\"injury_notes\")).alias(\"injury_notes\"),\n",
    "    \"college\",\n",
    "    when(col(\"years_exp\") == 1, True).otherwise(False).alias(\"is_rookie\")\n",
    ")\n",
    "\n",
    "df_joined = df_matchups_players_dim.join(\n",
    "    df_players_dim,\n",
    "    (df_matchups_players_dim.player_id == df_players_dim.player_id) &\n",
    "    (df_matchups_players_dim._league_id == df_players_dim._league_id) &\n",
    "    (df_matchups_players_dim._matchup_week == df_players_dim._matchup_week)\n",
    ")\n",
    "\n",
    "window_spec = Window.partitionBy(\n",
    "    df_matchups_players_dim[\"_league_id\"],\n",
    "    df_matchups_players_dim[\"_matchup_week\"],\n",
    "    \"player_position\"\n",
    ").orderBy(\"player_points\")\n",
    "\n",
    "df_joined = df_joined.withColumn(\"position_points_percentile\", F.percent_rank().over(window_spec))\n",
    "\n",
    "df_joined = df_joined.select(\n",
    "    \"roster_id\",\n",
    "    \"matchup_id\",\n",
    "    df_matchups_players_dim.player_id,\n",
    "    \"player_name\",\n",
    "    \"player_position\",\n",
    "    \"nfl_team\",\n",
    "    \"player_points\",\n",
    "    \"position_points_percentile\",\n",
    "    \"is_starter\",\n",
    "    \"years_exp\",\n",
    "    \"is_rookie\",\n",
    "    \"injury_status\",\n",
    "    \"injury_notes\",\n",
    "    \"college\",\n",
    "    df_matchups_players_dim._league_id,\n",
    "    df_matchups_players_dim._matchup_week,\n",
    "    df_matchups_players_dim._year\n",
    ")\n",
    "\n",
    "display(df_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72b1f9af-3c7d-42b1-87ba-bb17167d00cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.silver_matchups_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc6c2af0-5a03-4df0-8244-2106d3748504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_matchups_fact = spark.read.table('sleeper.silver_matchups_fact')\n",
    "df_rosters_dim = spark.read.table('sleeper.silver_rosters_dim')\n",
    "df_users_dim = spark.read.table('sleeper.silver_users_dim')\n",
    "\n",
    "df_result = df_matchups_fact.join(\n",
    "    df_rosters_dim,\n",
    "    (df_matchups_fact._league_id == df_rosters_dim._league_id) &\n",
    "    (df_matchups_fact.roster_id == df_rosters_dim.roster_id) &\n",
    "    (df_matchups_fact._matchup_week == df_rosters_dim._matchup_week)\n",
    ").join(\n",
    "    df_users_dim,\n",
    "    (df_matchups_fact._league_id == df_users_dim._league_id) &\n",
    "    (df_rosters_dim.owner_id == df_users_dim.owner_id) &\n",
    "    (df_matchups_fact._matchup_week == df_users_dim._matchup_week)\n",
    ").select(\n",
    "    \"matchup_id\",\n",
    "    df_matchups_fact.roster_id,\n",
    "    df_users_dim.owner_id,\n",
    "    df_users_dim.owner_name,\n",
    "    df_users_dim.is_commissioner,\n",
    "    df_users_dim.team_name,\n",
    "    df_matchups_fact.points,\n",
    "    df_rosters_dim.streak,\n",
    "    df_rosters_dim.record,\n",
    "    df_rosters_dim.wins,\n",
    "    df_rosters_dim.losses,\n",
    "    df_rosters_dim.ties,\n",
    "    df_rosters_dim.fpts,\n",
    "    df_rosters_dim.fpts_against,\n",
    "    df_rosters_dim.waiver_budget_used,\n",
    "    df_rosters_dim.waiver_position,\n",
    "    df_matchups_fact._league_id,\n",
    "    df_matchups_fact._matchup_week,\n",
    "    df_matchups_fact._year\n",
    ")\n",
    "\n",
    "display(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0028c222-c143-4d7b-bbf7-db73d2a25b32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.silver_model_roster_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1069416-7396-4999-8557-d81f4324b00a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.silver_model_player_performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "282d3872-f25c-4d3c-bbb3-6c45bef7f67b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "golden copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81b60070-80df-4fc3-8e4b-ce63ff833b1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType, MapType, DoubleType, NullType, FloatType, BooleanType, TimestampType, LongType\n",
    "from pyspark.sql.functions import col, expr, explode, current_timestamp, sum, when, lit, array_contains, coalesce, concat_ws\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "977c6d16-cd0c-4b6e-b635-69792e7547e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_roster_results = spark.read.table('sleeper.silver_model_roster_results')\n",
    "df_player_performances = spark.read.table('sleeper.silver_model_player_performances')\n",
    "\n",
    "# join rosters with player information to get complete breakdown of the matchup week\n",
    "df_joined_results = df_roster_results.alias(\"roster\").join(\n",
    "    df_player_performances.alias(\"performance\"),\n",
    "    (col(\"roster._league_id\") == col(\"performance._league_id\")) &\n",
    "    (col(\"roster.roster_id\") == col(\"performance.roster_id\")) &\n",
    "    (col(\"roster.matchup_id\") == col(\"performance.matchup_id\")) &\n",
    "    (col(\"roster._matchup_week\") == col(\"performance._matchup_week\"))\n",
    ")\n",
    "\n",
    "# aggregate the data at a roster level to get total amount of starter and roster points, to compute roster strength\n",
    "df_aggregated = df_joined_results.groupBy(\n",
    "    \"roster._league_id\", \"roster._matchup_week\",\"roster.matchup_id\", \"roster.roster_id\", \"roster.owner_id\", \"roster.owner_name\",\n",
    "    \"roster.is_commissioner\", \"roster.team_name\", \"roster.matchup_points\", \"roster.team_streak\",\n",
    "    \"roster.team_record\", \"roster.team_total_wins\", \"roster.team_total_losses\", \"roster.team_total_ties\",\n",
    "    \"roster.team_total_fpts\", \"roster.team_total_fpts_against\", \"roster.team_waiver_budget_used\",\n",
    "    \"roster.team_waiver_position\", \"roster._year\"\n",
    ").agg(\n",
    "    F.sum(F.when(col(\"performance.is_starter\"), col(\"performance.player_points\")).otherwise(0)).alias(\"starter_points\"),\n",
    "    F.sum(F.when(~col(\"performance.is_starter\"), col(\"performance.player_points\")).otherwise(0)).alias(\"bench_points\")\n",
    ").withColumn(\n",
    "    \"roster_strength\", col(\"starter_points\") + (0.2 * col(\"bench_points\"))\n",
    ")\n",
    "\n",
    "# rank the starter points against other teams in a given matchup week\n",
    "window_spec = Window.partitionBy(\"roster._league_id\",\"roster._matchup_week\",).orderBy(F.asc(\"matchup_points\"))\n",
    "df_aggregated = df_aggregated.withColumn(\"starter_points_percentile\", F.percent_rank().over(window_spec))\n",
    "\n",
    "# rank all the players for a given position on a roster to understand strong players at each position\n",
    "window_spec = Window.partitionBy(\"performance._league_id\",\"performance._matchup_week\", \"performance.roster_id\", \"performance.player_position\").orderBy(F.desc(\"performance.player_points\"))\n",
    "df_ranked = df_joined_results.withColumn(\"rank\", F.rank().over(window_spec))\n",
    "\n",
    "# get list of bench players\n",
    "df_bench_better_than_starters = df_ranked.filter(\n",
    "    ~col(\"performance.is_starter\")\n",
    ").select(\n",
    "    \"performance._league_id\", \"performance._matchup_week\", \"performance.roster_id\", \"performance.player_id\",\n",
    "    \"performance.player_name\", \"performance.player_position\", \"performance.player_points\"\n",
    ").alias(\"bench\")\n",
    "\n",
    "# get list of starter players\n",
    "df_starters = df_joined_results.filter(col(\"performance.is_starter\")).select(\n",
    "    \"performance._league_id\", \"performance._matchup_week\", \"performance.roster_id\", \"performance.player_position\",\n",
    "    col(\"performance.player_name\").alias(\"starter_player_name\"),\n",
    "    col(\"performance.player_points\").alias(\"starter_player_points\")\n",
    ").alias('starters')\n",
    "\n",
    "# join based on the matchup roster and player position to make suggestions about bench players that could have been starters\n",
    "df_bench_better_than_starters = df_bench_better_than_starters.alias(\"bench\").join(\n",
    "    df_starters.alias(\"starters\"),\n",
    "    (col(\"bench._league_id\") == col(\"starters._league_id\")) &\n",
    "    (col(\"bench._matchup_week\") == col(\"starters._matchup_week\")) &\n",
    "    (col(\"bench.roster_id\") == col(\"starters.roster_id\")) &\n",
    "    (col(\"bench.player_position\") == col(\"starters.player_position\"))\n",
    ").filter(\n",
    "    (col(\"bench.player_points\") > col(\"starters.starter_player_points\"))\n",
    ").withColumn(\n",
    "    \"point_oppertunity_cost\",\n",
    "    (col(\"bench.player_points\") - col(\"starters.starter_player_points\")).alias(\"point_opportunity_cost\")\n",
    ")\n",
    "\n",
    "# dedup bench players who get recommended twice (take the highest point potential)\n",
    "window_spec = Window.partitionBy(\"bench._league_id\",\"bench._matchup_week\", \"bench.roster_id\", \"bench.player_id\").orderBy(F.desc(\"point_oppertunity_cost\"))\n",
    "df_bench_better_than_starters = df_bench_better_than_starters.withColumn(\"bench_player_dedup\", F.rank().over(window_spec)).filter(\"bench_player_dedup == 1\")\n",
    "\n",
    "# create array of struct to list each suggestion\n",
    "df_bench_better_than_starters = df_bench_better_than_starters.select(\n",
    "    col(\"bench._league_id\"), col(\"bench._matchup_week\"), col(\"bench.roster_id\"),\n",
    "    F.struct(\n",
    "        col(\"bench.player_name\").alias(\"benched_player_name\"),\n",
    "        col(\"bench.player_points\").alias(\"benched_player_points\"),\n",
    "        col(\"starters.starter_player_name\"), col(\"starters.starter_player_points\"),\n",
    "        (col(\"bench.player_points\") - col(\"starters.starter_player_points\")).alias(\"point_opportunity_cost\")\n",
    "    ).alias(\"bench_better_than_starter\")\n",
    ")\n",
    "\n",
    "# collect list of bench better than starters and sum of their points (max points)\n",
    "df_bench_better_than_starters_agg = df_bench_better_than_starters.groupBy(\n",
    "    \"_league_id\", \"_matchup_week\", \"roster_id\"\n",
    ").agg(\n",
    "    F.collect_list(\"bench_better_than_starter\").alias(\"bench_better_than_starters\"),\n",
    "    F.sum(\"bench_better_than_starter.point_opportunity_cost\").alias(\"missed_starter_points\")\n",
    ")\n",
    "\n",
    "# join back to aggregated roster data\n",
    "df_aggregated = df_aggregated.alias(\"aggregated\").join(\n",
    "    df_bench_better_than_starters_agg.alias(\"bench_agg\"),\n",
    "    [\"_league_id\", \"_matchup_week\", \"roster_id\"], \"left\"\n",
    ")\n",
    "\n",
    "# get top 3 highest performing starters\n",
    "window_spec_highest_scoring = Window.partitionBy(\"roster._league_id\", \"roster._matchup_week\", \"roster.roster_id\").orderBy(F.desc(\"performance.player_points\"))\n",
    "df_highest_scoring = df_joined_results.filter(col(\"performance.is_starter\")).withColumn(\"rank\", F.row_number().over(window_spec_highest_scoring)).filter(col(\"rank\") <= 3)\n",
    "\n",
    "# create array of structs for top 3 highest scoring players\n",
    "df_highest_scoring_agg = df_highest_scoring.groupBy(\"roster._league_id\", \"roster._matchup_week\",\"roster.roster_id\").agg(\n",
    "    F.collect_list(\n",
    "        F.struct(\n",
    "            col(\"performance.player_name\").alias(\"highest_scoring_player_name\"),\n",
    "            col(\"performance.player_points\").alias(\"highest_scoring_player_points\")\n",
    "        )\n",
    "    ).alias(\"highest_scoring_players\")\n",
    ")\n",
    "\n",
    "# join back to roster data\n",
    "df_aggregated = df_aggregated.join(\n",
    "    df_highest_scoring_agg.alias(\"high_score_agg\"),\n",
    "    [\"_league_id\", \"_matchup_week\", \"roster_id\"], \"left\"\n",
    ")\n",
    "\n",
    "# get opponent points\n",
    "df_opponent_points = df_aggregated.select(\n",
    "    col(\"_league_id\"), \n",
    "    col(\"_matchup_week\"),\n",
    "    col(\"matchup_id\"), \n",
    "    col(\"roster_id\").alias(\"opponent_roster_id\"), \n",
    "    col(\"matchup_points\").alias(\"opponent_starter_points\"),\n",
    "    col(\"team_name\").alias(\"opponent_team_name\")\n",
    ")\n",
    "\n",
    "# self join to get opponent points and determine if missed starter points is enough to make a difference in the matchup. Determine managers efficiency based on ratio of starter points / max points\n",
    "df_aggregated = df_aggregated.alias(\"aggregated\").join(\n",
    "    df_opponent_points.alias(\"opponent\"),\n",
    "    (col(\"aggregated._league_id\") == col(\"opponent._league_id\")) &\n",
    "    (col(\"aggregated._matchup_week\") == col(\"opponent._matchup_week\")) &\n",
    "    (col(\"aggregated.matchup_id\") == col(\"opponent.matchup_id\")) &\n",
    "    (col(\"aggregated.roster_id\") != col(\"opponent.opponent_roster_id\")),\n",
    "    \"left\"\n",
    ").withColumn(\n",
    "    \"couldve_won_with_missed_bench_points\",\n",
    "    F.when(col(\"matchup_points\") > col(\"opponent_starter_points\"), None)\n",
    "    .when((col(\"matchup_points\") + col(\"missed_starter_points\")) > col(\"opponent_starter_points\"), True)\n",
    "    .otherwise(False)\n",
    ").withColumn(\n",
    "    \"manager_efficiency\",\n",
    "    col(\"starter_points\") / (col(\"starter_points\") + F.coalesce(col(\"missed_starter_points\"), F.lit(0)))\n",
    ").withColumn(\n",
    "    \"matchup_outcome\",\n",
    "    F.when(col(\"matchup_points\") > col(\"opponent_starter_points\"), \"Win\")\n",
    "    .when(col(\"matchup_points\") == col(\"opponent_starter_points\"), \"Draw\")\n",
    "    .otherwise(\"Loss\")\n",
    ").withColumn(\n",
    "    \"matchup_outcome_type\",\n",
    "    F.when((col(\"matchup_outcome\") == \"Win\") & (col(\"starter_points_percentile\") < 0.5), \"Lucky Win\")\n",
    "    .when((col(\"matchup_outcome\") == \"Win\") & (col(\"starter_points_percentile\") >= 0.5), \"Deserving Win\")\n",
    "    .when((col(\"matchup_outcome\") == \"Loss\") & (col(\"starter_points_percentile\") < 0.5), \"Deserving Loss\")\n",
    "    .when((col(\"matchup_outcome\") == \"Loss\") & (col(\"starter_points_percentile\") >= 0.5), \"Unlucky Loss\")\n",
    "    .when((col(\"matchup_outcome\") == \"Draw\") & (col(\"starter_points_percentile\") < 0.5), \"Low Scoring Draw\")\n",
    "    .when((col(\"matchup_outcome\") == \"Draw\") & (col(\"starter_points_percentile\") >= 0.5), \"High Scoring Draw\")\n",
    ")\n",
    "\n",
    "# generate percentile rank of the roster strength points to normalize the power points\n",
    "window_spec_percentile = Window.partitionBy(\"aggregated._league_id\", \"aggregated._matchup_week\").orderBy(\"roster_strength\")\n",
    "\n",
    "# compute power_points for the week based on roster strength percentile and manager efficiency\n",
    "df_aggregated = df_aggregated.withColumn(\"roster_strength_percentile\", F.percent_rank().over(window_spec_percentile)).withColumn(\n",
    "    \"week_power_points\",\n",
    "    (col(\"roster_strength_percentile\") * 0.85) + (col(\"manager_efficiency\") * 0.15)\n",
    ")\n",
    "\n",
    "return df_aggregated.select(\n",
    "    \"aggregated._league_id\", \"aggregated.matchup_id\", \"aggregated.roster_id\", \"aggregated.owner_id\",\n",
    "    \"aggregated.owner_name\", \"aggregated.is_commissioner\", \"aggregated.team_name\", \"aggregated.team_streak\",\n",
    "    \"aggregated.team_record\", \"aggregated.team_total_wins\", \"aggregated.team_total_losses\", \"aggregated.team_total_ties\",\n",
    "    \"aggregated.team_total_fpts\", \"aggregated.team_total_fpts_against\", \"aggregated.team_waiver_budget_used\",\n",
    "    \"aggregated.matchup_points\", \"starter_points\", \"starter_points_percentile\", \"bench_points\", \"bench_better_than_starters\", \"matchup_outcome\", \"matchup_outcome_type\",\n",
    "    \"missed_starter_points\", \"couldve_won_with_missed_bench_points\", \"opponent_starter_points\", \"opponent_team_name\", \"highest_scoring_players\",\n",
    "    \"roster_strength\", \"roster_strength_percentile\", \"manager_efficiency\", \"week_power_points\",\n",
    "    \"aggregated._matchup_week\", \"aggregated._year\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23671b2d-6ba0-49af-aff6-8982e1a6ec12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE sleeper.gold_power_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "210c3c27-5d38-4608-9367-369bab6473d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_aggregated.filter(df_aggregated._matchup_week == 14).select(\n",
    "    \"owner_name\",\n",
    "    \"roster_id\",\n",
    "    \"highest_scoring_players\",\n",
    "    \"bench_better_than_starters\",\n",
    "    \"couldve_won_with_missed_bench_points\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff85f30-a98e-4e10-b184-461bf42efae6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_roster_results = spark.read.table('sleeper.silver_model_roster_results')\n",
    "df_player_performances = spark.read.table('sleeper.silver_model_player_performances')\n",
    "\n",
    "df_joined_results = df_roster_results.alias(\"roster\").join(\n",
    "    df_player_performances.alias(\"performance\"),\n",
    "    (col(\"roster.roster_id\") == col(\"performance.roster_id\")) &\n",
    "    (col(\"roster.matchup_id\") == col(\"performance.matchup_id\")) &\n",
    "    (col(\"roster._matchup_week\") == col(\"performance._matchup_week\"))\n",
    ")\n",
    "\n",
    "df_aggregated = df_joined_results.groupBy(\n",
    "    \"roster.matchup_id\", \"roster.roster_id\", \"roster.owner_id\", \"roster.owner_name\",\n",
    "    \"roster.is_commissioner\", \"roster.team_name\", \"roster.matchup_points\", \"roster.team_streak\",\n",
    "    \"roster.team_record\", \"roster.team_total_wins\", \"roster.team_total_losses\", \"roster.team_total_ties\",\n",
    "    \"roster.team_total_fpts\", \"roster.team_total_fpts_against\", \"roster.team_waiver_budget_used\",\n",
    "    \"roster.team_waiver_position\", \"roster._league_id\", \"roster._matchup_week\", \"roster._year\"\n",
    ").agg(\n",
    "    F.sum(F.when(col(\"performance.is_starter\"), col(\"performance.player_points\")).otherwise(0)).alias(\"starter_points\"),\n",
    "    F.sum(F.when(~col(\"performance.is_starter\"), col(\"performance.player_points\")).otherwise(0)).alias(\"bench_points\")\n",
    ").withColumn(\n",
    "    \"roster_strength\", col(\"starter_points\") + (0.2 * col(\"bench_points\"))\n",
    ")\n",
    "\n",
    "window_spec = Window.partitionBy(\"performance.matchup_id\", \"performance.roster_id\", \"performance.player_position\").orderBy(F.desc(\"performance.player_points\"))\n",
    "df_ranked = df_joined_results.withColumn(\"rank\", F.rank().over(window_spec))\n",
    "\n",
    "df_bench_better_than_starters = df_ranked.filter(\n",
    "    (col(\"rank\") == 1) & (~col(\"performance.is_starter\"))\n",
    ").select(\n",
    "    \"performance.matchup_id\", \"performance.roster_id\", \"performance.player_id\",\n",
    "    \"performance.player_name\", \"performance.player_position\", \"performance.player_points\"\n",
    ")\n",
    "\n",
    "df_starters = df_joined_results.filter(col(\"performance.is_starter\")).select(\n",
    "    \"performance.matchup_id\", \"performance.roster_id\", \"performance.player_position\",\n",
    "    col(\"performance.player_name\").alias(\"starter_player_name\"),\n",
    "    col(\"performance.player_points\").alias(\"starter_player_points\")\n",
    ")\n",
    "\n",
    "df_bench_better_than_starters = df_bench_better_than_starters.alias(\"bench\").join(\n",
    "    df_starters.alias(\"starters\"),\n",
    "    (col(\"bench.matchup_id\") == col(\"starters.matchup_id\")) &\n",
    "    (col(\"bench.roster_id\") == col(\"starters.roster_id\")) &\n",
    "    (col(\"bench.player_position\") == col(\"starters.player_position\"))\n",
    ").select(\n",
    "    col(\"bench.matchup_id\"), col(\"bench.roster_id\"),\n",
    "    F.struct(\n",
    "        col(\"bench.player_name\").alias(\"benched_player_name\"),\n",
    "        col(\"bench.player_points\").alias(\"benched_player_points\"),\n",
    "        col(\"starters.starter_player_name\"), col(\"starters.starter_player_points\"),\n",
    "        (col(\"bench.player_points\") - col(\"starters.starter_player_points\")).alias(\"point_opportunity_cost\")\n",
    "    ).alias(\"bench_better_than_starter\")\n",
    ")\n",
    "\n",
    "df_bench_better_than_starters_agg = df_bench_better_than_starters.groupBy(\n",
    "    \"matchup_id\", \"roster_id\"\n",
    ").agg(\n",
    "    F.collect_list(\"bench_better_than_starter\").alias(\"bench_better_than_starters\"),\n",
    "    F.sum(\"bench_better_than_starter.point_opportunity_cost\").alias(\"missed_starter_points\")\n",
    ")\n",
    "\n",
    "df_aggregated = df_aggregated.alias(\"aggregated\").join(\n",
    "    df_bench_better_than_starters_agg.alias(\"bench_agg\"),\n",
    "    [\"matchup_id\", \"roster_id\"], \"left\"\n",
    ")\n",
    "\n",
    "window_spec_highest_scoring = Window.partitionBy(\"roster.matchup_id\", \"roster.roster_id\").orderBy(F.desc(\"performance.player_points\"))\n",
    "df_highest_scoring = df_joined_results.withColumn(\"rank\", F.row_number().over(window_spec_highest_scoring)).filter(col(\"rank\") <= 3)\n",
    "\n",
    "df_highest_scoring_agg = df_highest_scoring.groupBy(\"roster.matchup_id\", \"roster.roster_id\").agg(\n",
    "    F.collect_list(\n",
    "        F.struct(\n",
    "            col(\"performance.player_name\").alias(\"highest_scoring_player_name\"),\n",
    "            col(\"performance.player_points\").alias(\"highest_scoring_player_points\")\n",
    "        )\n",
    "    ).alias(\"highest_scoring_players\")\n",
    ")\n",
    "\n",
    "df_aggregated = df_aggregated.join(\n",
    "    df_highest_scoring_agg.alias(\"high_score_agg\"),\n",
    "    [\"matchup_id\", \"roster_id\"], \"left\"\n",
    ")\n",
    "\n",
    "df_opponent_points = df_aggregated.select(\n",
    "    col(\"matchup_id\"), col(\"roster_id\").alias(\"opponent_roster_id\"), col(\"starter_points\").alias(\"opponent_starter_points\")\n",
    ")\n",
    "\n",
    "df_aggregated = df_aggregated.alias(\"aggregated\").join(\n",
    "    df_opponent_points.alias(\"opponent\"),\n",
    "    (col(\"aggregated.matchup_id\") == col(\"opponent.matchup_id\")) &\n",
    "    (col(\"aggregated.roster_id\") != col(\"opponent.opponent_roster_id\")),\n",
    "    \"left\"\n",
    ").withColumn(\n",
    "    \"couldve_won_with_missed_bench_points\",\n",
    "    F.when(col(\"starter_points\") > col(\"opponent_starter_points\"), None)\n",
    "    .when((col(\"starter_points\") + col(\"missed_starter_points\")) > col(\"opponent_starter_points\"), True)\n",
    "    .otherwise(False)\n",
    ").withColumn(\n",
    "    \"manager_efficiency\",\n",
    "    col(\"starter_points\") / (col(\"starter_points\") + F.coalesce(col(\"missed_starter_points\"), F.lit(0)))\n",
    ")\n",
    "\n",
    "window_spec_percentile = Window.partitionBy(\"aggregated._league_id\", \"aggregated._matchup_week\").orderBy(\"roster_strength\")\n",
    "df_aggregated = df_aggregated.withColumn(\"roster_strength_percentile\", F.percent_rank().over(window_spec_percentile)).withColumn(\n",
    "    \"week_power_points\",\n",
    "    (col(\"roster_strength_percentile\") * 0.8) + (col(\"manager_efficiency\") * 0.2)\n",
    ")\n",
    "\n",
    "df_selected = df_aggregated.select(\n",
    "    \"aggregated._league_id\", \"aggregated.matchup_id\", \"aggregated.roster_id\", \"aggregated.owner_id\",\n",
    "    \"aggregated.owner_name\", \"aggregated.is_commissioner\", \"aggregated.team_name\", \"aggregated.team_streak\",\n",
    "    \"aggregated.team_record\", \"aggregated.team_total_wins\", \"aggregated.team_total_losses\", \"aggregated.team_total_ties\",\n",
    "    \"aggregated.team_total_fpts\", \"aggregated.team_total_fpts_against\", \"aggregated.team_waiver_budget_used\",\n",
    "    \"aggregated.matchup_points\", \"starter_points\", \"bench_points\", \"bench_better_than_starters\",\n",
    "    \"missed_starter_points\", \"couldve_won_with_missed_bench_points\", \"highest_scoring_players\",\n",
    "    \"roster_strength\", \"roster_strength_percentile\", \"manager_efficiency\", \"week_power_points\",\n",
    "    \"aggregated._matchup_week\", \"aggregated._year\"\n",
    ").dropDuplicates([\"_league_id\", \"roster_id\", \"_matchup_week\"])\n",
    "\n",
    "display(df_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60d7d0ab-bf1d-4d38-a452-f87c094b766e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.gold_weekly_performance_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07affa6-030b-4fd7-a5cc-2de245b82a63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.gold_weekly_performance_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b29bff0a-71af-449f-a28d-5ac30f17d0d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sleeper.gold_power_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e15e7456-1075-4dba-af89-e188e11e923a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  r.matchup_id, \n",
    "  r.roster_id, \n",
    "  r.owner_id, \n",
    "  r.owner_name,\n",
    "  r.is_commissioner, \n",
    "  r.team_name, \n",
    "  r.matchup_points, \n",
    "  r.team_streak,\n",
    "  r.team_record, \n",
    "  r.team_total_wins, \n",
    "  r.team_total_losses, \n",
    "  r.team_total_ties,\n",
    "  r.team_total_fpts, \n",
    "  r.team_total_fpts_against, \n",
    "  r.team_waiver_budget_used,\n",
    "  r.team_waiver_position, \n",
    "  r._league_id, \n",
    "  r._matchup_week, \n",
    "  r._year,\n",
    "  SUM(CASE WHEN p.is_starter THEN p.player_points ELSE 0 END) AS starter_points,\n",
    "  SUM(CASE WHEN NOT p.is_starter THEN p.player_points ELSE 0 END) AS bench_points,\n",
    "  SUM(CASE WHEN p.is_starter THEN p.player_points ELSE 0 END) + (0.2 * SUM(CASE WHEN NOT p.is_starter THEN p.player_points ELSE 0 END)) AS roster_strength\n",
    "FROM sleeper.silver_model_roster_results r\n",
    "  LEFT JOIN sleeper.silver_model_player_performances p\n",
    "    ON r.roster_id = p.roster_id\n",
    "    AND r._league_id = p._league_id\n",
    "    AND r._matchup_week = p._matchup_week\n",
    "GROUP BY \n",
    "  r.matchup_id, \n",
    "  r.roster_id, \n",
    "  r.owner_id, \n",
    "  r.owner_name,\n",
    "  r.is_commissioner, \n",
    "  r.team_name, \n",
    "  r.matchup_points, \n",
    "  r.team_streak,\n",
    "  r.team_record, \n",
    "  r.team_total_wins, \n",
    "  r.team_total_losses, \n",
    "  r.team_total_ties,\n",
    "  r.team_total_fpts, \n",
    "  r.team_total_fpts_against, \n",
    "  r.team_waiver_budget_used,\n",
    "  r.team_waiver_position, \n",
    "  r._league_id, \n",
    "  r._matchup_week, \n",
    "  r._year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1f70c3c-cd99-4266-93a8-26dd86f3f728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.gold_weekly_performance_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bd29815-7633-412e-96b4-7059edd9e729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# build exp decay method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02d17bdd-4147-47f9-9c3b-bdf1ab7b055f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "from pyspark.sql.functions import lit, col, rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "src = spark.read.table('sleeper.gold_weekly_performance_ranks')\n",
    "\n",
    "src = src.withColumn('power_rank_points', lit(None).cast(DoubleType())) \\\n",
    "    .withColumn('power_ranking', lit(None).cast(IntegerType())) \\\n",
    "    .withColumn('power_rank_change', lit(None).cast(IntegerType()))\n",
    "\n",
    "if spark.catalog.tableExists('sleeper.gold_power_rankings'):\n",
    "    target = spark.read.table('sleeper.gold_power_rankings')\n",
    "else:\n",
    "    target = spark.createDataFrame([], src.schema)\n",
    "\n",
    "# Alias columns in target\n",
    "target_alias = target.select(\n",
    "    col(\"_league_id\").alias(\"target_league_id\"),\n",
    "    col(\"_matchup_week\").alias(\"target_matchup_week\"),\n",
    "    col(\"roster_id\").alias(\"target_roster_id\")\n",
    ")\n",
    "\n",
    "# Perform the join\n",
    "src = src.join(\n",
    "    target_alias,\n",
    "    (src[\"_league_id\"] == target_alias[\"target_league_id\"]) &\n",
    "    (src[\"_matchup_week\"] == target_alias[\"target_matchup_week\"]) &\n",
    "    (src[\"roster_id\"] == target_alias[\"target_roster_id\"]),\n",
    "    how=\"left_anti\"\n",
    ").drop(\"target_league_id\", \"target_matchup_week\", \"target_roster_id\")\n",
    "\n",
    "distinct_league_week = src.select(\"_league_id\", \"_matchup_week\").distinct().orderBy(\"_league_id\", \"_matchup_week\")\n",
    "\n",
    "for row in distinct_league_week.collect():\n",
    "    league_id = row[\"_league_id\"]\n",
    "    week = row[\"_matchup_week\"]\n",
    "    print(f\"Processing league {league_id} week {week}\")\n",
    "\n",
    "    temp = src.filter((src[\"_league_id\"] == league_id) & (src[\"_matchup_week\"] == week))\n",
    "\n",
    "    previous_week = week - 1\n",
    "    previous_week_data = target.filter((target[\"_league_id\"] == league_id) & (target[\"_matchup_week\"] == previous_week)).alias(\"prev_week\")\n",
    "\n",
    "    if previous_week_data.count() > 0:\n",
    "        previous_week_data = previous_week_data \\\n",
    "            .select(\n",
    "                col('roster_id').alias('previous_roster_id'),\n",
    "                col(\"power_rank_points\").alias('previous_power_rank_points'),\n",
    "                col(\"power_ranking\").alias('previous_power_ranking')\n",
    "            )\n",
    "        temp = temp.join(previous_week_data, temp[\"roster_id\"] == previous_week_data[\"previous_roster_id\"], how=\"left\")\n",
    "        temp = temp.withColumn(\n",
    "            \"power_rank_points\",\n",
    "            (col(\"week_power_points\") * 0.3) + (col(\"previous_power_rank_points\") * 0.7)\n",
    "        )\n",
    "    else:\n",
    "        temp = temp \\\n",
    "            .withColumn(\"power_rank_points\", col(\"week_power_points\")) \\\n",
    "            .withColumn(\"power_rank_change\", lit(None))\n",
    "\n",
    "    # Calculate power_ranking\n",
    "    window_spec = Window.partitionBy(\"_league_id\").orderBy(col(\"power_rank_points\").desc())\n",
    "    temp = temp.withColumn(\"power_ranking\", rank().over(window_spec))\n",
    "\n",
    "    if previous_week_data.count() > 0:\n",
    "        temp = temp.withColumn(\n",
    "            \"power_rank_change\",\n",
    "            col('previous_power_ranking') - col('power_ranking')\n",
    "        )\n",
    "\n",
    "        temp = temp.drop('previous_power_rank_points').drop('previous_roster_id').drop('previous_power_ranking')\n",
    "\n",
    "    # Ensure schemas are compatible\n",
    "    for col_name, col_type in target.dtypes:\n",
    "        if col_name in temp.columns:\n",
    "            temp = temp.withColumn(col_name, temp[col_name].cast(col_type))\n",
    "\n",
    "    target = target.union(temp)\n",
    "\n",
    "display(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81e97a1-902e-44c3-8629-1ab6e930b603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "DROP TABLE sleeper.gold_power_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89a01e2a-59ff-4f0f-bcfe-504c1ff349b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT \n",
    "  owner_name,\n",
    "  is_commissioner,\n",
    "  team_name,\n",
    "  team_streak,\n",
    "  team_total_wins,\n",
    "  team_total_losses,\n",
    "  team_total_ties,\n",
    "  team_total_fpts,\n",
    "  team_total_fpts_against,\n",
    "  matchup_points,\n",
    "  missed_starter_points,\n",
    "  bench_better_than_starters,\n",
    "  couldve_won_with_missed_bench_points,\n",
    "  highest_scoring_players,\n",
    "  power_ranking,\n",
    "  power_rank_change\n",
    "FROM sleeper.gold_power_rankings WHERE `_matchup_week` = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50e70249-749d-4771-a797-603d027bb526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  owner_name,\n",
    "  is_commissioner,\n",
    "  team_name,\n",
    "  team_streak,\n",
    "  team_total_wins,\n",
    "  team_total_losses,\n",
    "  team_total_ties,\n",
    "  team_total_fpts,\n",
    "  team_total_fpts_against,\n",
    "  matchup_points,\n",
    "  missed_starter_points,\n",
    "  bench_better_than_starters,\n",
    "  couldve_won_with_missed_bench_points,\n",
    "  highest_scoring_players,\n",
    "  power_ranking,\n",
    "  power_rank_change\n",
    "FROM sleeper.gold_power_rankings WHERE `_matchup_week` = 14            \n",
    "\"\"\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c27a58d-e353-425f-9755-f99cdab3ec8c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table('sleeper.silver_users_dim')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8728f6ae-fb20-47ee-8f16-9ee790e98b2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = spark.read.table('sleeper.bronze_drafts_picks')\n",
    "\n",
    "df = df.select(\n",
    "    'draft_id',\n",
    "    'draft_slot',\n",
    "    'is_keeper',\n",
    "    'metadata.*',\n",
    "    'pick_no',\n",
    "    'picked_by',\n",
    "    'player_id',\n",
    "    'reactions',\n",
    "    'roster_id',\n",
    "    'round',\n",
    "    '_year',\n",
    "    '_league_id',\n",
    "    '_matchup_week',\n",
    "    '_ingested_ts'\n",
    ")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c2e83b2-6094-4f4e-bcd5-3ebd0bd1a399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, desc, col\n",
    "\n",
    "draft_picks_df = spark.read.table('sleeper.silver_drafts_picks_fact')\n",
    "\n",
    "users_df = spark.read.table('sleeper.silver_users_dim')\n",
    "\n",
    "window_spec = Window.partitionBy('_league_id', 'owner_id', '_matchup_week').orderBy(desc('__START_AT'))\n",
    "users_df = users_df.withColumn('rank', row_number().over(window_spec)).filter(col('rank') == 1).drop('rank')\n",
    "\n",
    "joined_df = draft_picks_df.alias('picks').join(\n",
    "    users_df.alias('users'),\n",
    "    (draft_picks_df['_league_id'] == users_df['_league_id']) &\n",
    "    (draft_picks_df['_matchup_week'] == users_df['_matchup_week']) &\n",
    "    (draft_picks_df['picked_by'] == users_df['owner_id']),\n",
    "    'inner'\n",
    ")\n",
    "\n",
    "joined_df = joined_df.select(\n",
    "    'draft_id',\n",
    "    'owner_id',\n",
    "    'owner_name',\n",
    "    'team_name',\n",
    "    'roster_id',\n",
    "    'draft_slot',\n",
    "    'round',\n",
    "    'pick_no',\n",
    "    'player_id',\n",
    "    'first_name',\n",
    "    'last_name',\n",
    "    'team',\n",
    "    'position',\n",
    "    'years_exp',\n",
    "    'injury_status',\n",
    "    'picks._league_id',\n",
    "    'picks._year',\n",
    "    'picks._matchup_week',\n",
    "    'picks._ingested_ts'\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "517d7f1c-0caf-4672-bcae-f160aa6f854e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2edbe5ad-b3b0-4cb3-afb2-3d0b28f98c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.gold_power_rankings ORDER BY roster_id DESC, _matchup_week ASC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f61f08d9-54ae-4fa8-a717-d7f24000e5d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_recursive_cte_sql(\n",
    "    source_table: str,\n",
    "    start_week: int,\n",
    "    end_week: int,\n",
    "    decay_weights=(0.3, 0.7),\n",
    "    output_table_name: str = \"power_rank_cte_view\"\n",
    ") -> str:\n",
    "    ctes = []\n",
    "\n",
    "    # Week 1: base case\n",
    "    ctes.append(f\"\"\"\n",
    "    week_{start_week} AS (\n",
    "        SELECT\n",
    "            _league_id,\n",
    "            _matchup_week,\n",
    "            roster_id,\n",
    "            week_power_points,\n",
    "            week_power_points AS power_rank_points\n",
    "        FROM {source_table}\n",
    "        WHERE _matchup_week = {start_week}\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    # Week 2 to N: apply decay using previous week's result\n",
    "    for week in range(start_week + 1, end_week + 1):\n",
    "        prev_week = week - 1\n",
    "        w_curr, w_prev = decay_weights\n",
    "        cte = f\"\"\"\n",
    "        week_{week} AS (\n",
    "            SELECT\n",
    "                curr._league_id,\n",
    "                curr._matchup_week,\n",
    "                curr.roster_id,\n",
    "                curr.week_power_points,\n",
    "                ({w_curr} * curr.week_power_points + {w_prev} * prev.power_rank_points) AS power_rank_points\n",
    "            FROM {source_table} curr\n",
    "            JOIN week_{prev_week} prev\n",
    "              ON curr._league_id = prev._league_id\n",
    "             AND curr.roster_id = prev.roster_id\n",
    "             AND curr._matchup_week = {week}\n",
    "        )\n",
    "        \"\"\"\n",
    "        ctes.append(cte)\n",
    "\n",
    "    # Final SELECT from all weeks unioned\n",
    "    union_all = \"\\nUNION ALL\\n\".join([f\"SELECT * FROM week_{w}\" for w in range(start_week, end_week + 1)])\n",
    "\n",
    "    sql = f\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW {output_table_name} AS\n",
    "    WITH\n",
    "    {',\\n'.join(ctes)}\n",
    "    {union_all}\n",
    "    \"\"\"\n",
    "\n",
    "    return sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19bd4037-cb86-4cb9-a1c9-fd5aaae43822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql_string = generate_recursive_cte_sql(\n",
    "    source_table=\"sleeper.gold_weekly_performance_ranks\",\n",
    "    start_week=13,\n",
    "    end_week=15,\n",
    "    decay_weights=(0.3, 0.7),\n",
    "    output_table_name=\"power_rank_cte_view\"\n",
    ")\n",
    "\n",
    "# Run it in Spark\n",
    "spark.sql(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b0f5546-1b0d-4335-9010-6dbed8d8eaf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(sql_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4d7b308-cc75-4f05-ab0d-7db359096202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW power_rank_cte_view AS\n",
    "WITH\n",
    "\n",
    "week_13 AS (\n",
    "    SELECT\n",
    "        _league_id,\n",
    "        _matchup_week,\n",
    "        roster_id,\n",
    "        week_power_points,\n",
    "        week_power_points AS power_rank_points\n",
    "    FROM sleeper.gold_weekly_performance_ranks\n",
    "    WHERE _matchup_week = 13\n",
    ")\n",
    ",\n",
    "\n",
    "    week_14 AS (\n",
    "        SELECT\n",
    "            curr._league_id,\n",
    "            curr._matchup_week,\n",
    "            curr.roster_id,\n",
    "            curr.week_power_points,\n",
    "            (0.3 * curr.week_power_points + 0.7 * prev.power_rank_points) AS power_rank_points\n",
    "        FROM sleeper.gold_weekly_performance_ranks curr\n",
    "        JOIN week_13 prev\n",
    "            ON curr._league_id = prev._league_id\n",
    "            AND curr.roster_id = prev.roster_id\n",
    "            AND curr._matchup_week = 14\n",
    "    )\n",
    "    ,\n",
    "\n",
    "    week_15 AS (\n",
    "        SELECT\n",
    "            curr._league_id,\n",
    "            curr._matchup_week,\n",
    "            curr.roster_id,\n",
    "            curr.week_power_points,\n",
    "            (0.3 * curr.week_power_points + 0.7 * prev.power_rank_points) AS power_rank_points\n",
    "        FROM sleeper.gold_weekly_performance_ranks curr\n",
    "        JOIN week_14 prev\n",
    "            ON curr._league_id = prev._league_id\n",
    "            AND curr.roster_id = prev.roster_id\n",
    "            AND curr._matchup_week = 15\n",
    "    )\n",
    "    \n",
    "SELECT * FROM week_13\n",
    "UNION ALL\n",
    "SELECT * FROM week_14\n",
    "UNION ALL\n",
    "SELECT * FROM week_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53a5a22f-82c1-452b-8725-e00db29fb156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM sleeper.gold_power_rankings ORDER BY roster_id ASC, _matchup_week ASC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d63ae1d-7292-456e-a5cb-0ff3eeafc9dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df\\\n",
    "  .groupBy(\"user\").agg(F.collect_list(\"raw_score\").alias(\"raw_score\"),F.collect_list(\"day\").alias(\"day\"))\\\n",
    "   .withColumn(\"raw_score1\", F.expr(\"\"\"transform(raw_score,(x,i)-> struct(x as raw,i as index))\"\"\"))\\\n",
    "   .withColumn(\"todays_score\", F.expr(\"\"\"transform(raw_score1, x-> aggregate(filter(raw_score1,z-> z.index<=x.index)\\\n",
    "                                             ,cast(0 as double),(acc,y)->(acc*0.9)+y.raw))\"\"\"))\\\n",
    "   .withColumn(\"zip\", F.explode(F.arrays_zip(\"day\",\"raw_score\",\"todays_score\")))\\\n",
    "   .select(\"user\", \"zip.*\")\\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc887b84-bffc-4d62-8949-ac220f228131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, collect_list, arrays_zip, explode\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = spark.read.table('sleeper.gold_weekly_performance_ranks')\n",
    "\n",
    "df = df\\\n",
    "  .groupBy(\"_league_id\",\"roster_id\").agg(F.collect_list(\"week_power_points\").alias(\"week_power_points\"),F.collect_list(\"_matchup_week\").alias(\"_matchup_week\"))\\\n",
    "    .withColumn(\"raw_score1\", F.expr(\"\"\"transform(week_power_points,(x,i)-> struct(x as raw,i as index))\"\"\"))\\\n",
    "    .withColumn(\"todays_score\", F.expr(\"\"\"transform(raw_score1, x-> aggregate(filter(raw_score1,z-> z.index<=x.index)\\\n",
    "                                                ,cast(0 as double),(acc,y)->(acc*0.9)+y.raw))\"\"\"))\\\n",
    "    .withColumn(\"zip\", F.explode(F.arrays_zip(\"roster_id\",\"_matchup_week\",\"week_power_points\",\"todays_score\")))\\\n",
    "   .select(\"user\", \"zip.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a08cb29-8da5-4876-a01a-4e47bdc4a54b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, collect_list, arrays_zip, explode\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = spark.read.table('sleeper.gold_weekly_performance_ranks')\n",
    "\n",
    "df = df.groupBy(\"_league_id\", \"roster_id\").agg(\n",
    "    F.collect_list(\"week_power_points\").alias(\"week_power_points\"),\n",
    "    F.collect_list(\"_matchup_week\").alias(\"_matchup_week\"),\n",
    "    F.collect_list(\"roster_id\").alias(\"roster_id_list\")\n",
    ").withColumn(\n",
    "    \"raw_score1\", \n",
    "    F.expr(\"\"\"transform(week_power_points, (x, i) -> struct(x as raw, i as index))\"\"\")\n",
    ").withColumn(\n",
    "    \"previous_week_score\", \n",
    "    F.expr(\"\"\"\n",
    "            transform(\n",
    "                raw_score1, \n",
    "                x-> \n",
    "                IF (x.index = 0, \n",
    "                    x.raw,\n",
    "                    aggregate(\n",
    "                        filter(\n",
    "                            raw_score1,\n",
    "                            z-> z.index + 1 <= x.index\n",
    "                        ),\n",
    "                        cast(0 as double),\n",
    "                        (acc,y)->(acc*.7)+(y.raw *.3)\n",
    "                    )\n",
    "                )\n",
    "            )\"\"\")\\\n",
    ").withColumn(\n",
    "    \"zip\", \n",
    "    F.explode(F.arrays_zip(\"roster_id_list\", \"_matchup_week\", \"week_power_points\", \"previous_week_score\"))\n",
    ").select('zip.*')\n",
    "\n",
    "display(df.filter('roster_id = 1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf368ad6-5d11-42af-a282-adadecb7cbeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame([\n",
    "                                 (0, 'a', 1),\n",
    "                                 (1, 'a', 1),\n",
    "                                 (2, 'a', 1),\n",
    "                                 (3, 'a', 1)],\n",
    "    ['day', 'user', 'raw_score']\n",
    ")\n",
    "\n",
    "df = df\\\n",
    "  .groupBy(\"user\").agg(F.collect_list(\"raw_score\").alias(\"raw_score\"),F.collect_list(\"day\").alias(\"day\"))\\\n",
    "   .withColumn(\"raw_score1\", F.expr(\"\"\"transform(raw_score,(x,i)-> struct(x as raw,i as index))\"\"\"))\\\n",
    "   .withColumn(\"todays_score\", F.expr(\"\"\"transform(raw_score1, x-> aggregate(filter(raw_score1,z-> z.index<=x.index)\\\n",
    "                                             ,cast(0 as double),(acc,y)->(acc*0.9)+y.raw))\"\"\"))\\\n",
    "   .withColumn(\"zip\", F.explode(F.arrays_zip(\"day\",\"raw_score\",\"todays_score\")))\\\n",
    "   .select(\"user\", \"zip.*\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6418e143-cacf-430e-8be0-a4c368c1804d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.table('sleeper.gold_weekly_performance_ranks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3174b4d-11c7-4dd1-9593-b0bdcda32e33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Adjust schema to match expected output exactly\n",
    "schema = StructType([\n",
    "    StructField(\"_league_id\", StringType(), True),\n",
    "    StructField(\"roster_id\", IntegerType(), True),\n",
    "    StructField(\"_matchup_week\", IntegerType(), True),\n",
    "    StructField(\"week_power_points\", DoubleType(), True),\n",
    "    StructField(\"previous_power_rank_score\", DoubleType(), True),\n",
    "    StructField(\"power_rank_score\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "pandas_udf(schema, functionType=PandasUDFType.GROUPED_MAP)\n",
    "\n",
    "def power_rankings(pdf):\n",
    "    pdf = pdf.sort_values(\"_matchup_week\").reset_index(drop=True)\n",
    "\n",
    "    prscore_list = []\n",
    "    prev_prscore_list = []\n",
    "\n",
    "    for i, row in pdf.iterrows():\n",
    "        if i == 0:\n",
    "            prscore = row[\"week_power_points\"]\n",
    "            prev_prscore = None\n",
    "        else:\n",
    "            prscore = prev_prscore * 0.7 + row[\"week_power_points\"] * 0.3\n",
    "        \n",
    "        prscore_list.append(prscore)\n",
    "        prev_prscore_list.append(prev_prscore)\n",
    "        prev_prscore = prscore\n",
    "\n",
    "    pdf[\"power_rank_score\"] = prscore_list\n",
    "    pdf[\"previous_power_rank_score\"] = prev_prscore_list\n",
    "\n",
    "    return pdf[[\"_league_id\", \"roster_id\", \"_matchup_week\", \"week_power_points\", \"power_rank_score\", \"previous_power_rank_score\"]]\n",
    "\n",
    "df = spark.read.table('sleeper.gold_weekly_performance_ranks')\n",
    "\n",
    "# Apply the function\n",
    "power_rank_df = df.groupby(\"_league_id\", \"roster_id\").applyInPandas(power_rankings, schema)\n",
    "\n",
    "joined_df = df.alias('src').join(\n",
    "    power_rank_df.alias('powerrank_df'), \n",
    "    on=[\"_league_id\", \"roster_id\", \"_matchup_week\", \"week_power_points\"]\n",
    ").select(\n",
    "    \"src.*\", \n",
    "    \"powerrank_df.power_rank_score\", \n",
    "    \"powerrank_df.previous_power_rank_score\"\n",
    ")\n",
    "\n",
    "# Define a window over each league and week\n",
    "rank_window = Window.partitionBy(\"_league_id\", \"_matchup_week\").orderBy(F.desc(\"power_rank_score\"))\n",
    "\n",
    "# Add the power rank (1 = best score)\n",
    "ranked_df = joined_df.withColumn(\"power_rank\", F.dense_rank().over(rank_window))\n",
    "\n",
    "prev_week_window = Window.partitionBy(\"_league_id\", \"roster_id\").orderBy(\"_matchup_week\")\n",
    "final_df = ranked_df.withColumn(\n",
    "    \"previous_power_rank\",\n",
    "    F.lag(\"power_rank\").over(prev_week_window)\n",
    ")\n",
    "\n",
    "display(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0051099e-f5d9-4485-84e7-72707b15211e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT _matchup_week, roster_id, team_name, power_rank, power_rank_change, power_rank_score, week_power_points, previous_power_rank_score, previous_power_rank, roster_strength_percentile, manager_efficiency, * FROM sleeper.gold_power_rankings \n",
    "--WHERE `_matchup_week` = 13\n",
    "ORDER BY roster_id, _matchup_week ASC"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4936567956729710,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "develop_models",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
